# research_tools.py (ä¿®æ­£ç‰ˆ)

import json
from pathlib import Path
from typing import Optional, List
import sys
import os
from datetime import datetime
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from fastmcp import FastMCP
from config.env_utils import ALi_API_KEY

mcp = FastMCP(name="research_server", instructions="æ£€ç´¢æŸ¥è¯¢mcpæœåŠ¡å™¨")

embeddings = DashScopeEmbeddings(model="text-embedding-v4", dashscope_api_key=ALi_API_KEY)

vectorstore_path = "/root/autodl-tmp/research_vectorstore"
os.makedirs(vectorstore_path, exist_ok=True)
METADATA_FILE = Path(vectorstore_path) / "knowledge_meta.json"

vectorstore = Chroma(persist_directory=vectorstore_path, embedding_function=embeddings)

# ===== å·¥å…·å®šä¹‰ =====
@mcp.tool(name="semantic_search", description="æ ¹æ®è¾“å…¥çš„æŸ¥è¯¢å†…å®¹ï¼Œè¿”å›æœ€ç›¸å…³çš„å†…å®¹")
async def semantic_search(query: str, top_k: int = 5) -> list:
    try:
        docs = vectorstore.similarity_search(query, k=top_k)
        results = []
        for i, doc in enumerate(docs):
            metadata = doc.metadata
            source = metadata.get('source', 'æœªçŸ¥æ¥æº')
            date = metadata.get('date', metadata.get('added_at', 'æœªçŸ¥æ—¥æœŸ'))
            results.append(
                f"ã€ç»“æœ {i + 1}ã€‘\n"
                f"æ¥æº: {source} | æ—¥æœŸ: {date}\n"
                f"å†…å®¹: {doc.page_content[:300]}...\n"
                f"{'-' * 50}"
            )
        return "\n".join(results)
    except Exception as e:
        return [f"æœç´¢å¤±è´¥: {str(e)}"]

@mcp.tool(name="add_to_knowledge_base", description="æ·»åŠ å†…å®¹åˆ°è¯­ä¹‰æœç´¢ä¸­")
def add_to_knowledge_base(
    text: str,
    source: str = "ç”¨æˆ·è¾“å…¥",
    category: str = "general",
    tags: Optional[List[str]] = None
) -> str:
    """
    æ·»åŠ å†…å®¹åˆ°è¯­ä¹‰æœç´¢ä¸­
    """
    global vectorstore
    try:
        metadata = {
            "source": source,
            "category": category,
            "tags": tags or [],
            "added_at": datetime.now().isoformat(),
            "text_length": len(text)
        }
        doc = Document(page_content=text, metadata=metadata)
        vectorstore.add_documents([doc])
        vectorstore.persist()

        # æ›´æ–°å…ƒæ•°æ®æ–‡ä»¶
        current_count = vectorstore._collection.count()
        meta_data = {
            "last_updated": datetime.now().isoformat(),
            "total_chunks": current_count
        }
        with open(METADATA_FILE, "w", encoding="utf-8") as f:
            json.dump(meta_data, f, ensure_ascii=False, indent=2)

        return f"âœ… æˆåŠŸæ·»åŠ æ–‡æ¡£\næ¥æº: {source}\né•¿åº¦: {len(text)} å­—ç¬¦"
    except Exception as e:
        return f"âŒ æ·»åŠ å¤±è´¥: {str(e)}"

@mcp.tool(name="list_knowledge_base_stats", description="æŸ¥çœ‹çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
def list_knowledge_base_stats() -> str:
    try:
        count = vectorstore._collection.count()
        last_updated = "æœªçŸ¥"
        if METADATA_FILE.exists():
            with open(METADATA_FILE, "r", encoding="utf-8") as f:
                meta = json.load(f)
                last_updated = meta.get("last_updated", "æœªçŸ¥")
        return (
            f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:\n"
            f"- æ–‡æ¡£ç‰‡æ®µæ€»æ•°: {count}\n"
            f"- æœ€åæ›´æ–°æ—¶é—´: {last_updated}\n"
            f"- å­˜å‚¨è·¯å¾„: {vectorstore_path}"
        )
    except Exception as e:
        return f"âŒ è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}"

@mcp.tool(name="ingest_document", description="ä¸Šä¼ å¹¶è§£æ PDF æˆ– DOCX æ–‡ä»¶ï¼Œå­˜å…¥çŸ¥è¯†åº“")
async def ingest_document(file_path: str, source_name: str = None) -> str:
    try:
        file_path = Path(file_path).resolve()
        if not file_path.exists():
            return "âŒ æ–‡ä»¶ä¸å­˜åœ¨"
        suffix = file_path.suffix.lower()
        if suffix == ".pdf":
            loader = PyPDFLoader(str(file_path))
        elif suffix == ".docx":
            loader = Docx2txtLoader(str(file_path))
        else:
            return "âŒ ä»…æ”¯æŒ .pdf å’Œ .docx æ–‡ä»¶"

        docs = loader.load()
        if not docs:
            return "âš ï¸ æ–‡æ¡£å†…å®¹ä¸ºç©º"

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        split_docs = splitter.split_documents(docs)

        metadata = {
            "source": source_name or file_path.name,
            "file_path": str(file_path),
            "ingested_at": datetime.now().isoformat(),
        }
        for doc in split_docs:
            doc.metadata.update(metadata)

        global vectorstore
        vectorstore.add_documents(split_docs)
        vectorstore.persist()

        # æ›´æ–°å…ƒæ•°æ®æ–‡ä»¶
        current_count = vectorstore._collection.count()
        meta_data = {
            "last_updated": datetime.now().isoformat(),
            "total_chunks": current_count
        }
        with open(METADATA_FILE, "w", encoding="utf-8") as f:
            json.dump(meta_data, f, ensure_ascii=False, indent=2)

        return f"âœ… æˆåŠŸè§£æå¹¶æ·»åŠ  {len(split_docs)} ä¸ªæ–‡æœ¬ç‰‡æ®µï¼ˆæ¥æº: {metadata['source']}ï¼‰"
    except Exception as e:
        import traceback
        print(f"[ERROR] ingest_document failed: {e}")
        traceback.print_exc()
        return f"âŒ è§£æå¤±è´¥: {str(e)}"

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨åŸºäº Qwen Embedding çš„ç ”ç©¶æœåŠ¡å™¨ (FastMCP)")
    print("ğŸ’¡ è¯·ç¡®ä¿å·²è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
    mcp.run()